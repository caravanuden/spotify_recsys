{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, Multiply, Reshape, Merge, Flatten, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import json, sys, random, os, datetime, math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Collaborative Filtering with NeuMF\n",
    "\n",
    "I am going to use only 1/10 of the data (100,000 sample playlists) for training due to time and resource constraints. This approach should scale if you have a server at your disposal. The NeuMF model for neural collaborative filtering combines both GMF and MLP (neural net) approaches. \n",
    "\n",
    "### Architecture for the NeuMF model:\n",
    "![NeuMF Architecture](img/neumf.png)\n",
    "Image taken from [He et al, 2017](https://arxiv.org/pdf/1708.05031.pdf), who developed this model and describe the approach in greater detail.\n",
    "\n",
    "### Steps\n",
    "1. Playlist (more generally called u for user) and item (i) vectors are used to create embeddings (low-dimensional) for each playlist and item.\n",
    "2. Generalized Matrix Factorization (GMF) combines the two embeddings using the dot product (this is the classic matrix factorization).\n",
    "3. Multi-layer perceptron (MLP) can also create embeddings for user and items. However, instead of taking a dot product of these to obtain the rating, I can concatenate them to create a feature vector that is passed on to deeper layers.\n",
    "4. NeuMF then combines the predictions from MLP and GMF to obtain the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def make_playlist_df(path, num_slices):\n",
    "    df_list = []\n",
    "    count = 0\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in sorted(filenames):\n",
    "        if count < num_slices and filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
    "            fullpath = os.sep.join((path, filename))\n",
    "            data = json.load(open(fullpath))\n",
    "            slice_df = pd.DataFrame.from_dict(data['playlists'], orient='columns')\n",
    "            df_list.append(slice_df)\n",
    "            count += 1\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "path = '../mpd.v1/data'\n",
    "traindata = make_playlist_df(path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also read in the challenge dataset which has missing songs\n",
    "# that I want the model to predict\n",
    "t = json.load(open('../challenge.v1/challenge_set.json'))\n",
    "challenge_df = pd.DataFrame.from_dict(t['playlists'], orient='columns')\n",
    "\n",
    "# combine train and challenge so can use cat code to map\n",
    "# track ids to an index 0-N across both datasets\n",
    "train_challengedata = pd.concat([traindata, challenge_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn playlist level dataframe into song level dataframe\n",
    "songPlaylistArray = []\n",
    "for index, row in train_challengedata.iterrows():\n",
    "    for track in row['tracks']:\n",
    "        songPlaylistArray.append([track['track_uri'], track['artist_name'], track['track_name'], row['pid'], row['num_holdouts']])\n",
    "songPlaylist = pd.DataFrame(songPlaylistArray, columns=['trackid', 'artist_name', 'track_name', 'pid', 'num_holdouts'])\n",
    "\n",
    "print(songPlaylist.shape)\n",
    "songPlaylist.head()   # is a df of all track ids, corresponding artist names, track names and playlist ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn songs into their unique cat codes so have a 0-N index for tracks\n",
    "songPlaylist['trackindex'] = songPlaylist['trackid'].astype('category').cat.codes\n",
    "print(len(songPlaylist['trackindex'].unique()))\n",
    "songPlaylist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and challenge data\n",
    "train = songPlaylist[pd.isnull(songPlaylist['num_holdouts'])]\n",
    "challenge = songPlaylist[pd.notnull(songPlaylist['num_holdouts'])]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# save data in dok matrix (optimized sparse matrix object)\n",
    "# create a sparse playlistid x trackindex matrix\n",
    "# if a playlistid i has song j, mat[i,j]=1\n",
    "mat = sp.dok_matrix((train.shape[0], len(songPlaylist['trackindex'].unique())), dtype=np.float32)\n",
    "for pid, trackindex in zip(train['pid'], train['trackindex']):\n",
    "    mat[pid, trackindex] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full NCF model\n",
    "def get_model(num_playlists, num_items, latent_dim=8, dense_layers=[64, 32, 16, 8],\n",
    "              reg_layers=[0, 0, 0, 0], reg_mf=0):\n",
    "\n",
    "    # input layer\n",
    "    input_user = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    input_item = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "    \n",
    "    # embedding layer\n",
    "    mf_user_embedding = Embedding(input_dim=num_playlists, output_dim=latent_dim,\n",
    "                        name='mf_user_embedding',\n",
    "                        embeddings_initializer='RandomNormal',\n",
    "                        embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    mf_item_embedding = Embedding(input_dim=num_items, output_dim=latent_dim,\n",
    "                        name='mf_item_embedding',\n",
    "                        embeddings_initializer='RandomNormal',\n",
    "                        embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    mlp_user_embedding = Embedding(input_dim=num_playlists, output_dim=int(dense_layers[0]/2),\n",
    "                         name='mlp_user_embedding',\n",
    "                         embeddings_initializer='RandomNormal',\n",
    "                         embeddings_regularizer=l2(reg_layers[0]), \n",
    "                         input_length=1)\n",
    "    mlp_item_embedding = Embedding(input_dim=num_items, output_dim=int(dense_layers[0]/2),\n",
    "                         name='mlp_item_embedding',\n",
    "                         embeddings_initializer='RandomNormal',\n",
    "                         embeddings_regularizer=l2(reg_layers[0]), \n",
    "                         input_length=1)\n",
    "\n",
    "    # MF latent vector\n",
    "    mf_user_latent = Flatten()(mf_user_embedding(input_user))\n",
    "    mf_item_latent = Flatten()(mf_item_embedding(input_item))\n",
    "    mf_cat_latent = Multiply()([mf_user_latent, mf_item_latent])\n",
    "\n",
    "    # MLP latent vector\n",
    "    mlp_user_latent = Flatten()(mlp_user_embedding(input_user))\n",
    "    mlp_item_latent = Flatten()(mlp_item_embedding(input_item))\n",
    "    mlp_cat_latent = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "    \n",
    "    mlp_vector = mlp_cat_latent\n",
    "    \n",
    "    # build dense layer for model\n",
    "    for i in range(1,len(dense_layers)):\n",
    "        layer = Dense(dense_layers[i],\n",
    "                      activity_regularizer=l2(reg_layers[i]),\n",
    "                      activation='relu',\n",
    "                      name='layer%d' % i)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    predict_layer = Concatenate()([mf_cat_latent, mlp_vector])\n",
    "    result = Dense(1, activation='sigmoid', \n",
    "                   kernel_initializer='lecun_uniform',name='result')\n",
    "\n",
    "    model = Model(inputs=[input_user,input_item], outputs=result(predict_layer))\n",
    "\n",
    "    return model\n",
    "\n",
    "# get the training samples\n",
    "def get_train_samples(train_mat, num_negatives):\n",
    "    playlist_input, item_input, labels = [], [], []\n",
    "    num_user, num_item = train_mat.shape\n",
    "    for (u, i) in train_mat.keys():\n",
    "        playlist_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_item)\n",
    "            while (u, j) in train_mat.keys():\n",
    "                j = np.random.randint(num_item)\n",
    "            playlist_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return playlist_input, item_input, labels\n",
    "\n",
    "# hyperparameters\n",
    "verbose = 1\n",
    "num_epochs = 1      # 10?\n",
    "batch_size = 256\n",
    "latent_dim = 8\n",
    "dense_layers = [64, 32, 16, 8]\n",
    "reg_layers = [0, 0, 0, 0]\n",
    "reg_mf = [0]\n",
    "num_negatives = 4\n",
    "learning_rate = 0.001\n",
    "learner = 'adam'\n",
    "dataset = 'spotify'\n",
    "\n",
    "# loading data\n",
    "train_mat = mat\n",
    "num_playlists, num_items = train_mat.shape\n",
    "print('Done loading data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# get model\n",
    "model = get_model(num_playlists, num_items, latent_dim, dense_layers, reg_layers, reg_mf)\n",
    "model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "    \n",
    "# train model\n",
    "# generate training instances\n",
    "playlist_input, item_input, labels = get_train_samples(train_mat, num_negatives)\n",
    "\n",
    "# training\n",
    "hist = model.fit([np.array(user_input), np.array(item_input)], np.array(labels), \n",
    "                 batch_size=batch_size, epochs=epochs, verbose=verbose, shuffle=True)\n",
    "print(hist.history)\n",
    "\n",
    "# save model\n",
    "model_file = '%s_NCF_%d_%s.h5' % (dataset, latent_dim, str(dense_layers))\n",
    "model.save(model_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
