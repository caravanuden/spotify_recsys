{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "import sys, re, json, os, datetime\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analyses of Spotify MPD\n",
    "\n",
    "![An example playist](img/playlist.png)\n",
    "\n",
    "An example of a Spotify playlist.\n",
    "\n",
    "### Dataset structure\n",
    "\n",
    "The data is structured in arrays that typically contains 1,000 playlists. Each playlist is a dictionary that contains the following fields:\n",
    "\n",
    "+ **pid** - integer - playlist id - the MPD ID of this playlist. This is an integer between 0 and 999,999.\n",
    "+ **name** - string - the name of the playlist\n",
    "+ **description** - optional string - if present, the description given to the playlist. Note that user-provided playlist descrptions are a relatively new feature of Spotify, so most playlists do not have descriptions.\n",
    "+ **modified_at** - seconds - timestamp (in seconds since the epoch) when this playlist was last updated. Times are rounded to midnight GMT of the date when the playlist was last updated.\n",
    "+ **num_artists** - the total number of unique artists for the tracks in the playlist.\n",
    "+ **num_albums** - the number of unique albums for the tracks in the playlist\n",
    "+ **num_tracks** - the number of tracks in the playlist\n",
    "+ **num_followers** - the number of followers this playlist had at the time the MPD was created. (Note that the follower count does not including the playlist creator)\n",
    "+ **num_edits** - the number of separate editing sessions. Tracks added in a two hour window are considered to be added in a single editing session.\n",
    "+ **duration_ms** - the total duration of all the tracks in the playlist (in milliseconds)\n",
    "+ **collaborative** - boolean - if true, the playlist is a collaborative playlist. Multiple users may contribute tracks to a collaborative playlist.\n",
    "+ **tracks** - an array of information about each track in the playlist. Each element in the array is a dictionary with the following fields:\n",
    "    - **track_name** - the name of the track\n",
    "    - **track_uri** - the Spotify URI of the track\n",
    "    - **album_name** - the name of the track's album\n",
    "    - **album_uri** - the Spotify URI of the album\n",
    "    - **artist_name** - the name of the track's primary artist\n",
    "    - **artist_uri** - the Spotify URI of track's primary artist\n",
    "    - **duration_ms** - the duration of the track in milliseconds\n",
    "    - **pos** - the position of the track in the playlist (zero-based)\n",
    "    \n",
    "### Some questions to guide my exploratory analyses\n",
    "\n",
    "1. This dataset was collected between January 2010 to October 2017. What were the top tracks, artists, and genres (measured through playlist titles) at this time?\n",
    "\n",
    "2. Are there natural patterns that fall out of clustering existing playlists? For example, are there playlist-level differences between different genre-specific playlists (such as rock, rap, classical) or \"mood\" playlists (happy, sad, studying)?\n",
    "\n",
    "3. Are there any differences between collaborative and \"personal\" playlists?\n",
    "\n",
    "Note that I removed the failed tests from the show and tell- you can see these in checkpoint.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "total_playlists = 0\n",
    "total_tracks = 0\n",
    "tracks = set()\n",
    "artists = set()\n",
    "albums = set()\n",
    "titles = set()\n",
    "total_descriptions = 0\n",
    "ntitles = set()\n",
    "title_histogram = Counter()\n",
    "artist_histogram = Counter()\n",
    "track_histogram = Counter()\n",
    "last_modified_histogram = Counter()\n",
    "num_edits_histogram = Counter()\n",
    "playlist_length_histogram = Counter()\n",
    "num_followers_histogram = Counter()\n",
    "\n",
    "def process_mpd(path):\n",
    "    count = 0\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in sorted(filenames):\n",
    "        if filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
    "            fullpath = os.sep.join((path, filename))\n",
    "            f = open(fullpath)\n",
    "            js = f.read()\n",
    "            f.close()\n",
    "            mpd_slice = json.loads(js)\n",
    "            process_info(mpd_slice['info'])\n",
    "            for playlist in mpd_slice['playlists']:\n",
    "                process_playlist(playlist)\n",
    "            count += 1\n",
    "\n",
    "def normalize_name(name):\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"[.,\\/#!$%\\^\\*;:{}=\\_`~()@-]\", ' ', name)\n",
    "    name = re.sub(r\"&\", 'and', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name\n",
    "\n",
    "def to_date(epoch):\n",
    "    return datetime.datetime.fromtimestamp(epoch).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def process_playlist(playlist):\n",
    "    global total_playlists, total_tracks, total_descriptions\n",
    "\n",
    "    total_playlists += 1\n",
    "    # print playlist['playlist_id'], playlist['name']\n",
    "\n",
    "    if 'description' in playlist:\n",
    "        total_descriptions += 1\n",
    "\n",
    "    titles.add(playlist['name'])\n",
    "    nname = normalize_name(playlist['name'])\n",
    "    ntitles.add(nname)\n",
    "    title_histogram[nname] += 1\n",
    "\n",
    "    playlist_length_histogram[playlist['num_tracks']] += 1\n",
    "    last_modified_histogram[playlist['modified_at']] += 1\n",
    "    num_edits_histogram[playlist['num_edits']] += 1\n",
    "    num_followers_histogram[playlist['num_followers']] += 1\n",
    "\n",
    "    for track in playlist['tracks']:\n",
    "        total_tracks += 1\n",
    "        albums.add(track['album_uri'])\n",
    "        tracks.add(track['track_uri'])\n",
    "        artists.add(track['artist_uri'])\n",
    "\n",
    "        full_name = track['track_name'] + \" by \" + track['artist_name']\n",
    "        artist_histogram[track['artist_name']] += 1\n",
    "        track_histogram[full_name] += 1\n",
    "\n",
    "\n",
    "def process_info(_):\n",
    "    pass\n",
    "\n",
    "path = '../mpd.v1/data'\n",
    "process_mpd(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1a\n",
    "\n",
    "This dataset was collected between January 2010 to October 2017. What were the top tracks, artists, and genres (measured through playlist titles) at this time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"number of playlists\", total_playlists)\n",
    "print (\"number of tracks\", total_tracks)\n",
    "print (\"number of unique tracks\", len(tracks))\n",
    "print (\"number of unique albums\", len(albums))\n",
    "print (\"number of unique artists\", len(artists))\n",
    "print (\"number of unique titles\", len(titles))\n",
    "print (\"number of playlists with descriptions\", total_descriptions)\n",
    "print (\"number of unique normalized titles\", len(ntitles))\n",
    "print (\"avg playlist length\", float(total_tracks) / total_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"top playlist titles\")\n",
    "for title, count in title_histogram.most_common(20):\n",
    "    print (\"%7d %s\" % (count, title))\n",
    "\n",
    "print ('\\n')\n",
    "print (\"top tracks\")\n",
    "for track, count in track_histogram.most_common(20):\n",
    "    print (\"%7d %s\" % (count, track))\n",
    "\n",
    "print ('\\n')\n",
    "print (\"top artists\")\n",
    "for artist, count in artist_histogram.most_common(20):\n",
    "    print (\"%7d %s\" % (count, artist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1b\n",
    "\n",
    "Let's also look at the distribution of number of edits, playlist length, and number of followers, and see if there are any interesting patterns there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_histogram(hist, num, var):\n",
    "    common = np.array(hist.most_common(num))\n",
    "    sns.distplot(common[:,0], hist_kws={\"weights\":common[:,1]})\n",
    "\n",
    "    plt.xlim(0, None)\n",
    "    plt.title(var)\n",
    "    plt.xlabel(' '.join(var.split()[:-1]))\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "show_histogram(num_edits_histogram, 20, 'Number of Edits Histogram')\n",
    "show_histogram(playlist_length_histogram, 20, 'Playlist Length Histogram')\n",
    "show_histogram(num_followers_histogram, 20, 'Number of Followers Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most people edit their playlists less than about 10 times. This makes sense, as each \"edit\" is a two-hour window of adding/removing songs, and most people just create an entirely new playlist instead of continually editing over the span of weeks or months. Additionally, most people have playlists that are between 15 and 30 songs in length, resulting in playlists that are about 90 minutes to 2 hours long. Additionally, he vast majority of playlists have 5 or fewer followers. Most playlists are quite personal and may serve to capture the specific songs of genre or mood that the user actually likes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Are there natural patterns that fall out of clustering existing playlists? For example, are there playlist-level differences between different genre-specific playlists (such as rock, rap, classical) or \"mood\" playlists (happy, sad, studying)?\n",
    "\n",
    "I'm going to look a few genres/moods: rock, rap/hip-hop, classical, pop, R&B, country, guilty pleasures, cardio/workout, party, happy, sad, studying, and angsty. I want to see if any natural clusters fall out of these genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def make_genre_df(path, num_slices, genres):\n",
    "    df_list = [[] for _ in range(len(genres))]\n",
    "    count = 0\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in sorted(filenames):\n",
    "        if count < num_slices and filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
    "            if count % 100 == 0:\n",
    "                print(count)\n",
    "            fullpath = os.sep.join((path, filename))\n",
    "            data = json.load(open(fullpath))\n",
    "            slice_df = json_normalize(data, 'playlists')\n",
    "            slice_df['normalized_name'] = slice_df['name'].apply(lambda x: normalize_name(x))\n",
    "            for i, genre in enumerate(genres):\n",
    "                genre_df = slice_df[slice_df['normalized_name'].str.match(genre)]\n",
    "                genre_df['genre'] = i\n",
    "                df_list[i].append(genre_df)\n",
    "            count += 1\n",
    "    df_list = [pd.concat(genre_df) for genre_df in df_list]\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "path = '../mpd.v1/data'\n",
    "genres = ['rock', '(?:rap|hip hop)', 'classical', 'pop', '(?:r and b|rhythm and blues)', 'country', 'guilty pleasures', '(?:cardio|workout)', 'party', 'happy', 'sad', 'angst', 'study']\n",
    "genre_df = make_genre_df(path, 1000, genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "feature_cols = ['duration_ms', 'num_albums', 'num_artists', 'num_edits', 'num_followers', 'num_tracks', 'collaborative']\n",
    "genre_df = genre_df.replace({'collaborative': {'false':0, 'true':1}})\n",
    "genre_df[feature_cols] = genre_df[feature_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "data = genre_df[feature_cols]\n",
    "labels = genre_df['genre']\n",
    "n_genres = genre_df['genre'].nunique()\n",
    "\n",
    "print('There are {0} genres.'.format(n_genres))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=n_genres, random_state=0)\n",
    "model.fit(data)\n",
    "\n",
    "model_metrics = metrics.precision_recall_fscore_support(labels, model.labels_)\n",
    "\n",
    "print('precision: {0}'.format(model_metrics[0]))\n",
    "print('recall: {0}'.format(model_metrics[1]))\n",
    "print('f-score: {0}'.format(model_metrics[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the f-scores of my genre kmeans clustering model, it seems that there are not playlist-level features (besides name, of course) that differentiate playlists from different genres. However, the rock, country, and party genres had the highest f-scores (of 0.13, 0.12, and 0.11, respectively) out of all of the genres. This surprised me, as I thought that genres like studying or classical would have more distinguishing features, as they tend to be longer, and I would think that they'd have more followers (Who wants to make a 5 hour playlist of background studying music? Not me).\n",
    "\n",
    "I was going to visual this clustering with PCA (to reduce feature dim), but my computer ran out of memory and my kernel died. To avoid having to load the genre dataframe again and destroying my computer, I'm going to end this analysis here. I've left my visualization code below if you'd like to give it a go. I think in the future it would be interesting to see if any genres naturally cluster together - for example, maybe rock and angst, classical and studying, or happy and pop might cluster together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I run into memory errors oh no\n",
    "# I tried to clear the genre df, but that killed my kernel\n",
    "# del genre_df\n",
    "\n",
    "# reduce the data w PCA for visualization\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "model = KMeans(n_clusters=n_genres, random_state=0)\n",
    "model.fit(reduced_data)\n",
    "\n",
    "# step size of the mesh, decrease to increase the quality of the VQ\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# plot the decision boundary\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# get labels for each point in mesh\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# put the result into a colored scatterplot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "\n",
    "# plot the cluster means\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=100, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on playlist genres (PCA-reduced data)\\n'\n",
    "          'Cluster means are marked with white X')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Are there any differences between collaborative and \"personal\" playlists? I hypothesize that collaborative playlists will have a greater diversity of artists, have more edits, and have more followers than personal playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def normalize_name(name):\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"[.,\\/#!$%\\^\\*;:{}=\\_`~()@-]\", ' ', name)\n",
    "    name = re.sub(r\"&\", 'and', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name\n",
    "\n",
    "def make_social_df(path, num_slices, cutoff):\n",
    "    df_list = []\n",
    "    count = 0\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in sorted(filenames):\n",
    "        if count < num_slices and filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
    "            fullpath = os.sep.join((path, filename))\n",
    "            data = json.load(open(fullpath))\n",
    "            slice_df = json_normalize(data, 'playlists')\n",
    "            slice_df['normalized_name'] = slice_df['name'].apply(lambda x: normalize_name(x))\n",
    "            popular_df = slice_df[slice_df['num_followers'] >= cutoff]\n",
    "            df_list.append(popular_df)\n",
    "            count += 1\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "path = '../mpd.v1/data'\n",
    "social_df = make_social_df(path, 1000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['duration_ms', 'num_albums', 'num_artists', 'num_edits', 'num_followers', 'num_tracks', 'collaborative']\n",
    "social_df = social_df.replace({'collaborative': {'false':0, 'true':1}})\n",
    "social_df[feature_cols] = social_df[feature_cols].apply(pd.to_numeric, errors='coerce')\n",
    "print(social_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I am only looking at playlists with more than 5 followers. Since I'm mostly looking at the social aspect of playlist creation here, I want to focus on playlists with more social engagement. Let's see what the playlists with the most followers, artists, and edits are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_df.sort_values(by=['num_followers'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_df.sort_values(by=['num_artists'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_df.sort_values(by=['num_edits'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There were {0} collaborative playlists.'.format(sum(social_df['collaborative'] == 1)))\n",
    "print('There were {0} non-collaborative playlists.'.format(sum(social_df['collaborative'] == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the histogram distributions, split by 'collaborative' class, of the numbers of followers, artists, and edits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_df.hist(column='num_followers')\n",
    "social_df.hist(column='num_followers', by='collaborative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_df.hist(column='num_artists')\n",
    "social_df.hist(column='num_artists', by='collaborative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_df.hist(column='num_edits')\n",
    "social_df.hist(column='num_edits', by='collaborative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like collaborative playlists have greater artist diversity and more edits, but not necessarily more followers. Let's see if these differences are statistically significant. I'll first plot some boxplots for each of number of edits, followers, and artists by collaboration class, and then do a Kruskal-Wallis one-way ANOVA to test for statistically significant differences between classes.\n",
    "\n",
    "Kruskal–Wallis is a non-parametric method to test whether or not the samples are drawn from the same distribution. Since it is a non-parametric method, the Kruskal–Wallis test does not assume a normal distribution of the residuals, unlike regular one-way ANOVA. Here we assume an identically shaped and scaled distribution for all groups, except for any difference in medians, giving us the null hypothesis that the medians of all groups are equal. Note that rejecting the null hypothesis does not indicate which of the groups differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='collaborative', y='num_edits', data=social_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='collaborative', y='num_artists', data=social_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='collaborative', y='num_followers', data=social_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "collab_social_df = social_df[social_df['collaborative'] == 1]\n",
    "personal_social_df = social_df[social_df['collaborative'] == 0]\n",
    "\n",
    "print('Statistical significance of edit count difference between collaborative/personal playlists:')\n",
    "print(stats.kruskal(personal_social_df['num_edits'], collab_social_df['num_edits']))\n",
    "\n",
    "print('\\nStatistical significance of unique artist count difference between collaborative/personal playlists:')\n",
    "print(stats.kruskal(personal_social_df['num_artists'], collab_social_df['num_artists']))\n",
    "\n",
    "print('\\nStatistical significance of follower count difference between collaborative/personal playlists:')\n",
    "print(stats.kruskal(personal_social_df['num_followers'], collab_social_df['num_followers']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kruskal-Wallis one-way ANOVA showed that there was a statistically significant difference in follower count (χ2(397) = 38.91, p < 0.00001), unique artist count (χ2(397) = 112.20, p < 0.00001), and edit count (χ2(397) = 69.59, p < 0.00001) between the two groups of collaborative vs personal playlists. Therefore, I reject the null hypothesis that the two groups share the same median follower, unique artist, and edit count and conclude that the groups significantly differ.\n",
    "\n",
    "Looking at the boxplots in combination with the Kruskal-Wallis ANOVAs, we see that collaborative playlists have more edits and unique artists, but fewer followers, than personal playlists. As expected, collaborative playlists have more edits (because more people are working on the same playlist- note, though, that the edit count is **not** double-, triple-, or higher-counting each additional collaborator, as it measures one edit session as one two-hour block when any edits occur in the playlist) and have more diverse artist lists (because different collaborators each contribute unique music tastes and artists to the playlist). Interestingly, collaborative playlists seem to have fewer followers than personal playlists, though this may be because there are many more personal (28718) than collaborative (933) playlists in this dataset.\n",
    "\n",
    "In light of these results, let's do a cross-validated logistic regression to see if we can predict playlist collaboration from the playlist-level features we've been working with. I'm going to look quickly at the variance inflation factors for these features to see if they're highly correlated (which might cause issues for my log regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "df_w_constant = add_constant(social_df[feature_cols])\n",
    "\n",
    "pd.Series([variance_inflation_factor(df_w_constant.values, i) for i in range(df_w_constant.shape[1])], index=df_w_constant.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the variance inflation factors for the selected features, we can see that the number of albums, artists, tracks, and playlist duration are all highly correlated, with VIFs > 10 (obviously, because playlists with more tracks will be longer, and playlists with more artists will have more albums). I'll remove the playlist length and number of albums features to remove this multicollinearity in my logistic regression model (see below that this eliminates most of the multicollinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['num_artists', 'num_edits', 'num_followers', 'num_tracks', 'collaborative']\n",
    "\n",
    "df_w_constant = add_constant(social_df[feature_cols])\n",
    "\n",
    "pd.Series([variance_inflation_factor(df_w_constant.values, i) for i in range(df_w_constant.shape[1])], index=df_w_constant.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, roc_curve, auc\n",
    "\n",
    "training_features = ['num_artists', 'num_edits', 'num_followers', 'num_tracks']\n",
    "\n",
    "X = social_df[training_features]\n",
    "y = social_df['collaborative']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "sm = SMOTE(random_state=0)\n",
    "X_train_smote, y_train_smote = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionCV(Cs=10, random_state=1)\n",
    "fitted = model.fit(X_train_smote, y_train_smote)\n",
    "y_pred = fitted.predict(X_test)\n",
    "\n",
    "print('The model selected {0} as C.'.format(fitted.C_))\n",
    "print('The most informative features are:')\n",
    "print(sorted(zip(fitted.coef_[0], X.columns.values),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "\n",
    "print('Precision, recall, and F1 score, averaged and weighted by number of instances in each class:')\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('f1 score: {}\\n'.format(fscore))\n",
    "\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "print('Precision, recall, and F1 score, per class [0 1]:')\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('f1 score: {}'.format(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fpr, tpr, thresholds and roc auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = np.trapz(tpr,fpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression classifier didn't work that well; even though during training I tried to account for class imbalance by generating artificial samples with SMOTE, it seems that the playlist-level features aren't the most informative. My model has an overall f-score of 0.747, which hides the class imbalance issue of an f-score of 0.07 in the minority (collaborative) class. Additionally, my ROC AUC is only 0.558 (chance is 0.5).\n",
    "\n",
    "Looking at the coefficients of my model, though, number of artists is the most informative feature, followed by the number of edits, tracks, and followers respectively. Note that after removing my multicollinear features (num of albums and playlist duration) I drastically improved my model (in checkpoint.ipynb I didn't remove these features, resulting in an f-score of only 0.24 and a ROC AUC of 0.532)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
